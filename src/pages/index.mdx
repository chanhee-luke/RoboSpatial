---
layout: ../layouts/Layout.astro
title: "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics"
description: Webpage for RoboSpatial
favicon: favicon.ico
# thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

{/* Image imports */}
import fig1 from "../assets/fig1_new.png";
import fig2 from "../assets/fig2.png";
import fig3 from "../assets/fig3.png";
import robot_exp from "../assets/robot_exp.png";



<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Chan Hee Song",
      url: "https://chanh.ee",
      institution: "The Ohio State University",
      notes: ["*"],
    },
    {
      name: "Valts Blukis",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/valts-blukis",
      notes: [],
    },
    {
      name: "Jonathan Tremblay",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/jonathan-tremblay",
      notes: [],
    },
    {
      name: "Stephen Tyree",
      institution: "Nvidia",
      url: "https://research.nvidia.com/person/stephen-tyree",
    },
    {
      name: "Yu Su",
      url: "https://ysu1989.github.io/",
      institution: "The Ohio State University",
    },
    {
      name: "Stan Birchfield",
      url: "https://research.nvidia.com/person/stan-birchfield",
      institution: "Nvidia",
    },
  ]}
  conference="CVPR 2025"
  notes={[
    {
      symbol: "*",
      text: "Work done during Nvidia internship",
    },
  ]}
  links={[
    {
      name: "ArXiv",
      url: "https://arxiv.org/abs/2411.16537",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code (TBA)",
      url: "",
      icon: "mdi:github",
      disabled: true,
    },
    {
      name: "Dataset (TBA)",
      url: "",
      icon: "mdi:database",
      disabled: true
    },
  ]}
  />

<div className="center-text">
  **TL;DR** A large-scale 2D/3D dataset of real indoor environments for spatial reasoning in robotics.
</div>


{/* <Video source={outside} /> */}

<HighlightedSection>

## Overview

{/* **TL;DR** 1) A new training dataset and benchmark, **RoboSpatial** and **RoboSpatial-Home**, comprising images and 3D scans paired with spatial questions and answers, designed for robotics. 2) **RoboSpatial** trained model outperforms prior SOTA VLMs on natural language-specified robot manipulation tasks and indoor spatial scene question answering. */}

Spatial understanding is essential for robots to perceive, reason about, and interact with their environments. However, current visual language models often rely on general-purpose image datasets that lack robust spatial scene understanding and reference frame comprehension (ego-, object-, or world-centric). To address this gap, we introduce **RoboSpatial**, a large-scale dataset of **real indoor and tabletop environments** captured via egocentric images and 3D scans. RoboSpatial provides **1M images, 5K 3D scans, and 3M annotated spatial relationships**, enabling both 2D and 3D spatial reasoning. Models trained on RoboSpatial outperform baselines on tasks including spatial affordance prediction, spatial relationship prediction, and robotics manipulation.

</HighlightedSection>

## RoboSpatial Application Example


<Figure
    caption="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning."
  >
    <Image source={fig1} altText="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning." />
</Figure>


{/* ## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns> */}

## Approach

<Figure
    caption="We automatically generate spatial relationship annotations from existing datasets with 3D point clouds, egocentric images, and 3D bounding box annotations. We create question/answer pairs covering three classes of spatial relationships, three spatial reference frames, and both binary (yes/no) and numeric (e.g. 2D image points) answers. From 1M images and 5K scans, we generate over 3M spatial question/answer pairs."
  >
    <Image source={fig2} altText="An overview of RoboSpatial Dataset." />
</Figure>



## Evaluation Results

### Spatial QA

<Figure
    caption="Results of RoboSpatial-trained models on RoboSpatial test set, RoboSpatial-Home and BLINK. Two models are shown: SL (SpaceLLaVA) and RP (RoboPoint), where the S- prefix and FT indicates RoboSpatial-trained models. For Yes/No questions, the green checkmark indicates the correct answer. For spatial context questions, GT indicates the correct answer. For in-domain, all questions are from RoboSpatial test. For out-of-domain, all images except for the top right are from RoboSpatial-Home."
  >
    <Image source={fig3} altText="Qualitative Results of RoboSpatial-trained models." />
</Figure>

<Table caption="Performance of RoboSpatial-trained models on three spatial reasoning benchmarks.">
| Model                       | RoboSpatial Test | RoboSpatial-Home | BLINK-Spatial |
|:----------------------------|:---------------:|:---------------:|:---------------:|
| **Open-source**             |                |                 |                 |
| *--2D--*             |                |                 |                 |
| LLaVA-NeXT                  | 30.3           |23.7           |71.8           |
| + RoboSpatial               | 60.5    |  42.0               |    **79.0**             |
| RoboPoint                  | 38.9           |40.2           |  63.6           |
| + RoboSpatial               | 70.6    |  50.7               |    70.6             |
| *--3D--*             |                |                 |                 |
| Embodied Generalist        |     42.8     |       36.2          |     N/A            |
| + RoboSpatial               | **71.9**    |      **54.7**       |    N/A             |
| **Baselines**               |            |                 |                 |
| Molmo                     | 50.1           |   46.9        |   67.1          |
| GPT-4o                   | 50.8           |   48.6        |    76.2         |
</Table>

### Robot Manipulation

<Figure
    caption="Robotics experiments: the red dot shows the model output (if not present, the model failed to provide a valid point in the image);  green dots are used to show when a model outputs multiple points. The robot motion generator, cuRobo, is used to grasp the item referenced by the generated point. The spatial- prefix indicates model trained with RoboSpatial."
  >
    <Image source={robot_exp} altText="Robot experiment results." />
</Figure>

<Table caption="Task success rate for robot manipulation.">
| Model                        | Success Rate (%) |
|:----------------------------|:---------------:|
| **Open-source**              |                 |
| LLaVA-NeXT               | 23.7           |
| + RoboSpatial                | **52.6**    |
| **Baselines** |            |
| Molmo                     | 43.8           |
| GPT-4o                   | 46.9           |
</Table>

{/* | RoboPoint                 | 44.7           |
| + RoboSpatial                | 46.2        | */}


## BibTeX Citation

```bibtex
@inproceedings{song2025robospatial,
  author    = {Song, Chan Hee and Blukis, Valts and Tremblay, Jonathan and Tyree, Stephen and Su, Yu and Birchfield, Stan},
  title     = {{RoboSpatial}: Teaching Spatial Understanding to {2D} and {3D} Vision-Language Models for Robotics},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025},
  note      = {To appear},
}
```